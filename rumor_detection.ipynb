{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS631.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51ce580c13fb4d5bae1a99525321bef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_842d412640f6418aa8e78e5f1760b555",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a0e6481b49ce40f4946c0d729810d4b6",
              "IPY_MODEL_61fe4cfd40b948579c5bbfa73ddab92c"
            ]
          }
        },
        "842d412640f6418aa8e78e5f1760b555": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0e6481b49ce40f4946c0d729810d4b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9938732a5dcf441eb95058df1fe5512c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244715968,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244715968,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57b279137f4b413ea3191963599094b9"
          }
        },
        "61fe4cfd40b948579c5bbfa73ddab92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa08b9021d4f43b4b5f34167043abaca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 245M/245M [00:13&lt;00:00, 18.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69897f7e544c45b48a97602e842f2cfb"
          }
        },
        "9938732a5dcf441eb95058df1fe5512c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57b279137f4b413ea3191963599094b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa08b9021d4f43b4b5f34167043abaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69897f7e544c45b48a97602e842f2cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-npRL-5iGVxB"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "# !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GjUfKebJKUC",
        "outputId": "3b78ba8b-d308-4c7c-a52f-46b9a24dfce6"
      },
      "source": [
        "!pip install sentence-transformers -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 4.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 11.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 60.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 54.3MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 61.0MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vXGs2s8GcWy"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(appName=\"YourTest\", master=\"local[*]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGg5teccRC8S"
      },
      "source": [
        "# import sparknlp \n",
        "# from sparknlp.pretrained import PretrainedPipeline\n",
        "# spark = sparknlp.start()\n",
        "\n",
        "# print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "# print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0kW-UUmGdfi"
      },
      "source": [
        "!wget https://ndownloader.figshare.com/files/16188500 -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX5-H6gAmeOk"
      },
      "source": [
        "# We don't need these. I added glove and bert vectors lower down.\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImeC_-dfnm3V"
      },
      "source": [
        "# !gunzip ./GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mXFmN_RnFT2"
      },
      "source": [
        "# from gensim import models\n",
        "# w = models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ0NiylZxtVN",
        "outputId": "90baee17-989d-4cae-9eef-2c4c05fa332f"
      },
      "source": [
        "!tar -xvf 16188500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rumoureval2019/\n",
            "rumoureval2019/final-eval-key.json\n",
            "rumoureval2019/LICENSE\n",
            "rumoureval2019/home_scorer_macro.py\n",
            "rumoureval2019/README\n",
            "rumoureval2019/rumoureval-2019-training-data.zip\n",
            "rumoureval2019/rumoureval-2019-test-data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iec4rGMxy9A"
      },
      "source": [
        "!unzip -qn rumoureval2019/rumoureval-2019-training-data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b8fb-u9gc2B"
      },
      "source": [
        "#### IMPORT ####\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "# from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, expr, explode, struct, regexp_replace, collect_list, lit\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession, Row\n",
        "# Don't import all here, it overrides the sum function\n",
        "# from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import * \n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "import re\n",
        "from pyspark.ml.linalg import VectorUDT, Vectors\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iPB8PKxJY26"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"YourTest\").getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYmWqfEuGeC9"
      },
      "source": [
        "#### DOWNLOAD SOURCE TWEETS && REPLY TWEETS ###\n",
        "path = \"./rumoureval-2019-training-data/twitter-english/*/*/source-tweet/*.json\"\n",
        "source_tweets_df = spark.read.json(path)\n",
        "path = \"./rumoureval-2019-training-data/twitter-english/*/*/replies/*.json\"\n",
        "reply_tweets_df = spark.read.json(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd85ixPKCvX3",
        "outputId": "ed37ebfd-97e2-423f-b37f-5d2129329935"
      },
      "source": [
        "print(reply_tweets_df.schema)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StructType(List(StructField(contributors,StringType,true),StructField(coordinates,StructType(List(StructField(coordinates,ArrayType(DoubleType,true),true),StructField(type,StringType,true))),true),StructField(created_at,StringType,true),StructField(entities,StructType(List(StructField(hashtags,ArrayType(StructType(List(StructField(indices,ArrayType(LongType,true),true),StructField(text,StringType,true))),true),true),StructField(media,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(media_url,StringType,true),StructField(media_url_https,StringType,true),StructField(sizes,StructType(List(StructField(large,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(medium,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(small,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(thumb,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true))),true),StructField(source_status_id,LongType,true),StructField(source_status_id_str,StringType,true),StructField(source_user_id,LongType,true),StructField(source_user_id_str,StringType,true),StructField(type,StringType,true),StructField(url,StringType,true))),true),true),StructField(symbols,ArrayType(StringType,true),true),StructField(urls,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(url,StringType,true))),true),true),StructField(user_mentions,ArrayType(StructType(List(StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(name,StringType,true),StructField(screen_name,StringType,true))),true),true))),true),StructField(extended_entities,StructType(List(StructField(media,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(media_url,StringType,true),StructField(media_url_https,StringType,true),StructField(sizes,StructType(List(StructField(large,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(medium,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(small,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(thumb,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true))),true),StructField(source_status_id,LongType,true),StructField(source_status_id_str,StringType,true),StructField(source_user_id,LongType,true),StructField(source_user_id_str,StringType,true),StructField(type,StringType,true),StructField(url,StringType,true),StructField(video_info,StructType(List(StructField(aspect_ratio,ArrayType(LongType,true),true),StructField(variants,ArrayType(StructType(List(StructField(bitrate,LongType,true),StructField(content_type,StringType,true),StructField(url,StringType,true))),true),true))),true))),true),true))),true),StructField(favorite_count,LongType,true),StructField(favorited,BooleanType,true),StructField(geo,StructType(List(StructField(coordinates,ArrayType(DoubleType,true),true),StructField(type,StringType,true))),true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(in_reply_to_screen_name,StringType,true),StructField(in_reply_to_status_id,LongType,true),StructField(in_reply_to_status_id_str,StringType,true),StructField(in_reply_to_user_id,LongType,true),StructField(in_reply_to_user_id_str,StringType,true),StructField(is_quote_status,BooleanType,true),StructField(lang,StringType,true),StructField(place,StructType(List(StructField(bounding_box,StructType(List(StructField(coordinates,ArrayType(ArrayType(ArrayType(DoubleType,true),true),true),true),StructField(type,StringType,true))),true),StructField(contained_within,ArrayType(StringType,true),true),StructField(country,StringType,true),StructField(country_code,StringType,true),StructField(full_name,StringType,true),StructField(id,StringType,true),StructField(name,StringType,true),StructField(place_type,StringType,true),StructField(url,StringType,true))),true),StructField(possibly_sensitive,BooleanType,true),StructField(possibly_sensitive_appealable,BooleanType,true),StructField(quoted_status,StructType(List(StructField(contributors,StringType,true),StructField(coordinates,StringType,true),StructField(created_at,StringType,true),StructField(entities,StructType(List(StructField(hashtags,ArrayType(StructType(List(StructField(indices,ArrayType(LongType,true),true),StructField(text,StringType,true))),true),true),StructField(media,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(media_url,StringType,true),StructField(media_url_https,StringType,true),StructField(sizes,StructType(List(StructField(large,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(medium,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(small,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(thumb,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true))),true),StructField(type,StringType,true),StructField(url,StringType,true))),true),true),StructField(symbols,ArrayType(StringType,true),true),StructField(urls,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(url,StringType,true))),true),true),StructField(user_mentions,ArrayType(StringType,true),true))),true),StructField(extended_entities,StructType(List(StructField(media,ArrayType(StructType(List(StructField(additional_media_info,StructType(List(StructField(monetizable,BooleanType,true))),true),StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(media_url,StringType,true),StructField(media_url_https,StringType,true),StructField(sizes,StructType(List(StructField(large,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(medium,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(small,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true),StructField(thumb,StructType(List(StructField(h,LongType,true),StructField(resize,StringType,true),StructField(w,LongType,true))),true))),true),StructField(type,StringType,true),StructField(url,StringType,true),StructField(video_info,StructType(List(StructField(aspect_ratio,ArrayType(LongType,true),true),StructField(duration_millis,LongType,true),StructField(variants,ArrayType(StructType(List(StructField(bitrate,LongType,true),StructField(content_type,StringType,true),StructField(url,StringType,true))),true),true))),true))),true),true))),true),StructField(favorite_count,LongType,true),StructField(favorited,BooleanType,true),StructField(geo,StringType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(in_reply_to_screen_name,StringType,true),StructField(in_reply_to_status_id,StringType,true),StructField(in_reply_to_status_id_str,StringType,true),StructField(in_reply_to_user_id,StringType,true),StructField(in_reply_to_user_id_str,StringType,true),StructField(is_quote_status,BooleanType,true),StructField(lang,StringType,true),StructField(place,StringType,true),StructField(possibly_sensitive,BooleanType,true),StructField(retweet_count,LongType,true),StructField(retweeted,BooleanType,true),StructField(source,StringType,true),StructField(text,StringType,true),StructField(truncated,BooleanType,true),StructField(user,StructType(List(StructField(contributors_enabled,BooleanType,true),StructField(created_at,StringType,true),StructField(default_profile,BooleanType,true),StructField(default_profile_image,BooleanType,true),StructField(description,StringType,true),StructField(entities,StructType(List(StructField(description,StructType(List(StructField(urls,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(url,StringType,true))),true),true))),true),StructField(url,StructType(List(StructField(urls,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(url,StringType,true))),true),true))),true))),true),StructField(favourites_count,LongType,true),StructField(follow_request_sent,BooleanType,true),StructField(followers_count,LongType,true),StructField(following,BooleanType,true),StructField(friends_count,LongType,true),StructField(geo_enabled,BooleanType,true),StructField(has_extended_profile,BooleanType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(is_translation_enabled,BooleanType,true),StructField(is_translator,BooleanType,true),StructField(lang,StringType,true),StructField(listed_count,LongType,true),StructField(location,StringType,true),StructField(name,StringType,true),StructField(notifications,BooleanType,true),StructField(profile_background_color,StringType,true),StructField(profile_background_image_url,StringType,true),StructField(profile_background_image_url_https,StringType,true),StructField(profile_background_tile,BooleanType,true),StructField(profile_banner_url,StringType,true),StructField(profile_image_url,StringType,true),StructField(profile_image_url_https,StringType,true),StructField(profile_link_color,StringType,true),StructField(profile_sidebar_border_color,StringType,true),StructField(profile_sidebar_fill_color,StringType,true),StructField(profile_text_color,StringType,true),StructField(profile_use_background_image,BooleanType,true),StructField(protected,BooleanType,true),StructField(screen_name,StringType,true),StructField(statuses_count,LongType,true),StructField(time_zone,StringType,true),StructField(translator_type,StringType,true),StructField(url,StringType,true),StructField(utc_offset,LongType,true),StructField(verified,BooleanType,true))),true))),true),StructField(quoted_status_id,LongType,true),StructField(quoted_status_id_str,StringType,true),StructField(retweet_count,LongType,true),StructField(retweeted,BooleanType,true),StructField(source,StringType,true),StructField(text,StringType,true),StructField(truncated,BooleanType,true),StructField(user,StructType(List(StructField(contributors_enabled,BooleanType,true),StructField(created_at,StringType,true),StructField(default_profile,BooleanType,true),StructField(default_profile_image,BooleanType,true),StructField(description,StringType,true),StructField(entities,StructType(List(StructField(description,StructType(List(StructField(urls,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(url,StringType,true))),true),true))),true),StructField(url,StructType(List(StructField(urls,ArrayType(StructType(List(StructField(display_url,StringType,true),StructField(expanded_url,StringType,true),StructField(indices,ArrayType(LongType,true),true),StructField(url,StringType,true))),true),true))),true))),true),StructField(favourites_count,LongType,true),StructField(follow_request_sent,BooleanType,true),StructField(followers_count,LongType,true),StructField(following,BooleanType,true),StructField(friends_count,LongType,true),StructField(geo_enabled,BooleanType,true),StructField(has_extended_profile,BooleanType,true),StructField(id,LongType,true),StructField(id_str,StringType,true),StructField(is_translation_enabled,BooleanType,true),StructField(is_translator,BooleanType,true),StructField(lang,StringType,true),StructField(listed_count,LongType,true),StructField(location,StringType,true),StructField(name,StringType,true),StructField(notifications,BooleanType,true),StructField(profile_background_color,StringType,true),StructField(profile_background_image_url,StringType,true),StructField(profile_background_image_url_https,StringType,true),StructField(profile_background_tile,BooleanType,true),StructField(profile_banner_url,StringType,true),StructField(profile_image_url,StringType,true),StructField(profile_image_url_https,StringType,true),StructField(profile_link_color,StringType,true),StructField(profile_location,StringType,true),StructField(profile_sidebar_border_color,StringType,true),StructField(profile_sidebar_fill_color,StringType,true),StructField(profile_text_color,StringType,true),StructField(profile_use_background_image,BooleanType,true),StructField(protected,BooleanType,true),StructField(screen_name,StringType,true),StructField(statuses_count,LongType,true),StructField(time_zone,StringType,true),StructField(translator_type,StringType,true),StructField(url,StringType,true),StructField(utc_offset,LongType,true),StructField(verified,BooleanType,true))),true)))\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Yie1iqgJtz"
      },
      "source": [
        "#### DOWNLOAD TRUE LABELS ###\n",
        "schema = StructType([StructField(\"subtaskaenglish\", MapType(StringType(), StringType())),StructField(\"subtaskbenglish\", MapType(StringType(), StringType()))])\n",
        "dev_key = spark.read.schema(schema).option(\"multiline\", \"true\").json('rumoureval-2019-training-data/dev-key.json')\n",
        "train_key = spark.read.schema(schema).option(\"multiline\", \"true\").json('rumoureval-2019-training-data/train-key.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6RDX4-n2bpp"
      },
      "source": [
        "# TRUE LABELS FOR TASK A\n",
        "dev_key_taskA = dev_key.select(explode(col(\"subtaskaenglish\")))\n",
        "train_key_taskA = train_key.select(explode(col(\"subtaskaenglish\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wARSp4215cFV",
        "outputId": "8f6715c8-de14-4797-a528-cb97ceba18eb"
      },
      "source": [
        "dev_key_taskA.take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(key='553556756857884673', value='comment'),\n",
              " Row(key='552801161128853504', value='comment'),\n",
              " Row(key='758315535570444290', value='comment'),\n",
              " Row(key='498334502041956352', value='query'),\n",
              " Row(key='758334112662904834', value='deny')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUSDkipyMhxQ"
      },
      "source": [
        "# reply_tweets_df.withColumn('used',lit(\"0\")).select('used').take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR7pZbUBQUE4"
      },
      "source": [
        "# reply_tweets_df.withColumn('set',lit('unknown')).select('set').take(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAj4xyyN8Auh"
      },
      "source": [
        "# reply_tweets_df.join(dev_key_taskA, reply_tweets_df.id_str == dev_key_taskA.key, 'left').drop('key').withColumnRenamed('value', 'label').select('id_str','label').take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVJbPoxxuPYA"
      },
      "source": [
        "# TRUE LABELS FOR TASK B\n",
        "dev_key_taskB = dev_key.select(explode(col(\"subtaskbenglish\")))\n",
        "train_key_taskB = train_key.select(explode(col(\"subtaskbenglish\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rEde7whfPpK"
      },
      "source": [
        "# import sparknlp\n",
        "# spark = SparkSession.builder.appName(\"cs631\").config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.7.5:3.1.1\").getOrCreate()\n",
        "# spark = sparknlp.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSQ8EGlmgJT8"
      },
      "source": [
        "### DATA CLEANING  ##\n",
        "def clean(df):\n",
        "\n",
        "    def replace_url(text):\n",
        "        return re.sub(r'https?:\\/\\/.*[\\r\\n]*', 'url_url_url', text, flags=re.MULTILINE)\n",
        "\n",
        "    replace_url_udf = udf(replace_url, StringType())\n",
        "\n",
        "    df = df.withColumn('cleaned_text', replace_url_udf(col('text')))\n",
        "\n",
        "    ### REMOVE @\n",
        "    df = df.withColumn('cleaned_text', regexp_replace(col('cleaned_text'), r'(@([A-Za-z0-9]+))', ''))\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVQldSgJVNZ6"
      },
      "source": [
        "words_dict = dict(\n",
        "      has_belief_words = set(\"assume believe apparent per-haps suspect think thought consider\".split()),\n",
        "      has_report_words = set(\"evidence source official footage capture assert told claim according\".split()),\n",
        "      has_doubt_words = set(\"wonder allege unsure guess speculate doubt\".split()),\n",
        "      has_knowledge = set(\"confirm definitely admit\".split()),\n",
        "      has_denial_words = set(\"refuse reject rebuff dismiss contradict oppose\".split()),\n",
        "      has_curse_words = set(\"lol rofl lmfao yeah stfu aha wtf shit\".split()),\n",
        "      # negation_words = set(\"no not no one nothing never donâ€™t canâ€™t hardly\".split(),\n",
        "      has_question_words = set(\"when which what who how whom why whose\".split()),\n",
        "      has_other_words = set(\"irresponsible careless liar false witness untrue neglect integrity murder fake\".split())\n",
        "      # has_false_synonyms = set(['false',  'bogus',  'deceitful',  'dishonest',\n",
        "      #                 'distorted',  'erroneous',  'fake', 'fanciful',\n",
        "      #                 'faulty',  'fictitious',  'fraudulent',\n",
        "      #                 'improper',  'inaccurate',  'incorrect',\n",
        "      #                 'invalid', 'misleading', 'mistaken', 'phony',\n",
        "      #                 'specious', 'spurious', 'unfounded', 'unreal',\n",
        "      #                 'untrue',  'untruthful',  'apocryphal',  'beguiling',\n",
        "      #                 'casuistic',  'concocted', 'cooked-up',\n",
        "      #                 'counterfactual', 'deceiving', 'delusive', 'ersatz',\n",
        "      #                 'fallacious', 'fishy',  'illusive',  'imaginary',\n",
        "      #                 'inexact',  'lying',  'mendacious',\n",
        "      #                 'misrepresentative', 'off the mark', 'sham',\n",
        "      #                 'sophistical', 'trumped up', 'unsound']),\n",
        "      # has_false_antonyms = set(['accurate', 'authentic', 'correct', 'fair', 'faithful',\n",
        "      #                 'frank', 'genuine', 'honest', 'moral', 'open', 'proven',\n",
        "      #                 'real', 'right', 'sincere', 'sound', 'true',\n",
        "      #                 'trustworthy', 'truthful', 'valid', 'actual', 'factual',\n",
        "      #                 'just', 'known', 'precise', 'reliable', 'straight',\n",
        "      #                 'substantiated'])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAsYSTgGhqrS"
      },
      "source": [
        "## FEATURE EXTRACTION ##\n",
        "\n",
        "def extract_features(df):\n",
        "    hasqmark = udf(lambda x: int('?' in x), IntegerType())\n",
        "    df = df.withColumn('hasqmark', hasqmark(col('cleaned_text')))\n",
        "\n",
        "    hasmark = udf(lambda x: int('!' in x), IntegerType())\n",
        "    df = df.withColumn('hasmark', hasmark(col('cleaned_text')))\n",
        "\n",
        "    hasperiod = udf(lambda x: int('.' in x), IntegerType())\n",
        "    df = df.withColumn('hasperiod', hasperiod(col('cleaned_text')))\n",
        "\n",
        "    df = df.withColumn('hashtags_count', expr('size(entities.hashtags)'))\n",
        "\n",
        "    df = df.withColumn('mentions_count', expr('size(entities.user_mentions)'))\n",
        "\n",
        "    df = df.withColumn('hasurls', expr('cast(size(entities.urls) >= 1 AS int)'))\n",
        "\n",
        "    df = df.withColumn('hasmedia', expr('cast(size(entities.media) >= 1 AS int)'))\n",
        "\n",
        "    df = df.withColumn('friends_count', expr('user.friends_count'))\n",
        "\n",
        "    df = df.withColumn('followers_count', expr('user.followers_count'))\n",
        "\n",
        "    ratiocapital = udf(lambda x: sum(map(str.isupper, x))/(len(x)+1), FloatType())\n",
        "    df = df.withColumn('ratiocapital', ratiocapital(col('cleaned_text')))\n",
        "\n",
        "    charlen = udf(lambda x: len(x), IntegerType())\n",
        "    df = df.withColumn('charlen', charlen(col('cleaned_text')))\n",
        "\n",
        "    df = df.withColumn('issource', expr('CAST((in_reply_to_status_id IS NULL) AS INT)'))\n",
        "\n",
        "    ## TOKENIZATION ##\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
        "    temp_df = tokenizer.transform(df)\n",
        "\n",
        "    #TODO remove stop words?\n",
        "    #--eric--\n",
        "\n",
        "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "    df = remover.transform(temp_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawhashtf\", numFeatures=100)\n",
        "    #hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawhashtf\", numFeatures=100)\n",
        "    #--eric--\n",
        "    df = hashingTF.transform(df)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawhashtf\", outputCol=\"hashtf\")\n",
        "    idfModel = idf.fit(df)\n",
        "    df = idfModel.transform(df)\n",
        "    # TODO join with df so it has hashtf column\n",
        "\n",
        "    ## TOKEN FEATURE EXTRACTION ##\n",
        "    wordlen = udf(lambda words: len(words), IntegerType())\n",
        "    df = df.withColumn('wordlen', wordlen(col('words')))\n",
        "\n",
        "    def contains(y, x):\n",
        "      return int(bool(len(y.intersection(set(x)))))\n",
        "\n",
        "\n",
        "    for name, ys in words_dict.items():\n",
        "        df = df.withColumn(name, udf(partial(contains, ys), IntegerType())(col('words')))\n",
        "\n",
        "\n",
        "\n",
        "    #TODO negation words etc.\n",
        "    negationwords = ['not', 'no', 'nobody', 'nothing', 'none', 'never',\n",
        "              'neither', 'nor', 'nowhere', 'hardly', 'scarcely',\n",
        "              'barely', 'don', 'isn', 'wasn', 'shouldn', 'wouldn',\n",
        "              'couldn', 'doesn']\n",
        "    def negacount(words):\n",
        "      c = 0\n",
        "      for negationword in negationwords:\n",
        "        if negationword in words:\n",
        "          c += 1\n",
        "      return c\n",
        "    negationcount = udf(negacount, IntegerType())\n",
        "    df = df.withColumn('hasnegation', negationcount(col('words')))\n",
        "\n",
        "    @udf('float')\n",
        "    def count_upper(x):\n",
        "        a = x.split()\n",
        "        return sum(map(str.isupper, a))/(len(a) + 1)\n",
        "\n",
        "    df = df.withColumn('allcapsratio', count_upper(col('cleaned_text')))\n",
        "    #eric\n",
        "\n",
        "    ## SENTENCE VECTORS  ##\n",
        "    ## GLOVE ##\n",
        "    # model = SentenceTransformer('average_word_embeddings_glove.6B.300d').eval()\n",
        "    # temp = (df.select(['id','cleaned_text']).collect())\n",
        "\n",
        "    # j = model.encode([x.cleaned_text for x in temp])\n",
        "\n",
        "    # glove_rows = [Row(id=temp[i]['id'], glove=Vectors.dense(j[i])) for i in range(len(j))]\n",
        "\n",
        "    # glove_schema = StructType([\n",
        "    #   StructField(\"id\", LongType(), True),\n",
        "    #   StructField(\"glove_vector\", VectorUDT(), True)\n",
        "    # ])\n",
        "\n",
        "    # d_df = spark.createDataFrame(glove_rows, glove_schema)\n",
        "\n",
        "    # df  = df.join(d_df, on='id')\n",
        "\n",
        "    ## BERT ##\n",
        "    model = SentenceTransformer('stsb-distilbert-base').eval()\n",
        "\n",
        "    temp = (df.select(['id','cleaned_text']).collect())\n",
        "\n",
        "    j = model.encode([x.cleaned_text for x in temp], convert_to_tensor=True)\n",
        "\n",
        "    bert_rows = [Row(id=temp[i]['id'], glove=Vectors.dense(j[i])) for i in range(len(j))]\n",
        "\n",
        "    bert_schema = StructType([\n",
        "      StructField(\"id\", LongType(), True),\n",
        "      StructField(\"bert_vector\", VectorUDT(), True)\n",
        "    ])\n",
        "\n",
        "    d_df = spark.createDataFrame(bert_rows, bert_schema)\n",
        "\n",
        "    df  = df.join(d_df, on='id')\n",
        "\n",
        "    ### other models SentenceTransformer\n",
        "    # model = SentenceTransformer('stsb-roberta-large').eval()\n",
        "\n",
        "    # temp = (df.select(['id','cleaned_text']).collect())\n",
        "\n",
        "    # j = model.encode([x.cleaned_text for x in temp])\n",
        "\n",
        "    # bert_rows = [Row(id=temp[i]['id'], glove=Vectors.dense(j[i])) for i in range(len(j))]\n",
        "\n",
        "    # bert_schema = StructType([\n",
        "    #   StructField(\"id\", LongType(), True),\n",
        "    #   StructField(\"bert_vector_other_large\", VectorUDT(), True)\n",
        "    # ])\n",
        "\n",
        "    # d_df = spark.createDataFrame(bert_rows, bert_schema)\n",
        "\n",
        "    # df  = df.join(d_df, on='id')\n",
        "\n",
        "    ### base\n",
        "    # model = SentenceTransformer('stsb-roberta-base').eval()\n",
        "\n",
        "    # temp = (df.select(['id','cleaned_text']).collect())\n",
        "\n",
        "    # j = model.encode([x.cleaned_text for x in temp], convert_to_tensor=True)\n",
        "\n",
        "    # bert_rows = [Row(id=temp[i]['id'], glove=Vectors.dense(j[i])) for i in range(len(j))]\n",
        "\n",
        "    # bert_schema = StructType([\n",
        "    #   StructField(\"id\", LongType(), True),\n",
        "    #   StructField(\"bert_vector_other_base\", VectorUDT(), True)\n",
        "    # ])\n",
        "\n",
        "    # d_df = spark.createDataFrame(bert_rows, bert_schema)\n",
        "\n",
        "    # df  = df.join(d_df, on='id')\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "51ce580c13fb4d5bae1a99525321bef8",
            "842d412640f6418aa8e78e5f1760b555",
            "a0e6481b49ce40f4946c0d729810d4b6",
            "61fe4cfd40b948579c5bbfa73ddab92c",
            "9938732a5dcf441eb95058df1fe5512c",
            "57b279137f4b413ea3191963599094b9",
            "aa08b9021d4f43b4b5f34167043abaca",
            "69897f7e544c45b48a97602e842f2cfb"
          ]
        },
        "id": "aE8qPBtUpzh8",
        "outputId": "2342e853-5af8-42ca-d6d6-5dbc7cb743bd"
      },
      "source": [
        "source_preprocessed = extract_features(clean(source_tweets_df))\n",
        "reply_preprocessed = extract_features(clean(reply_tweets_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51ce580c13fb4d5bae1a99525321bef8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244715968.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWvGvIb-Vz0Z",
        "outputId": "4dc250ab-19c5-471e-d4cd-1152d18a2b80"
      },
      "source": [
        "print(source_preprocessed.count())\n",
        "print(reply_preprocessed.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "325\n",
            "5243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoQdDeL8JfWG"
      },
      "source": [
        "# temp = source_preprocessed.selectExpr('id AS in_reply_to_status_id', 'glove_vector AS glove_vector_source')\n",
        "# reply_preprocessed = reply_preprocessed.join(temp, 'in_reply_to_status_id')\n",
        "\n",
        "temp = source_preprocessed.selectExpr('id AS in_reply_to_status_id', 'bert_vector AS bert_vector_source')\n",
        "reply_preprocessed = reply_preprocessed.join(temp, 'in_reply_to_status_id')\n",
        "\n",
        "# temp = source_preprocessed.selectExpr('id AS in_reply_to_status_id', 'bert_vector_other_large AS bert_vector_other_large_source')\n",
        "# reply_preprocessed = reply_preprocessed.join(temp, 'in_reply_to_status_id')\n",
        "\n",
        "# temp = source_preprocessed.selectExpr('id AS in_reply_to_status_id', 'bert_vector_other_base AS bert_vector_other_base_source')\n",
        "# reply_preprocessed = reply_preprocessed.join(temp, 'in_reply_to_status_id')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR4PvP9QkWfE"
      },
      "source": [
        "# I think this is duplicated, arent we already removing mentions in the cleaning section?\n",
        "#eric\n",
        "#add cleaning special characters : '@'\n",
        "# from pyspark.sql.functions import regexp_replace, col\n",
        "# reply_preprocessed = reply_preprocessed.withColumn(\"cleaned_text\",regexp_replace(col(\"cleaned_text\"), \"@\", \"\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqN56rHiEh48"
      },
      "source": [
        "# Flattening is done with the assembler in the pipeline below\n",
        "#eric\n",
        "#flatten the labels into one column\n",
        "# reply_preprocessed_rdd = reply_preprocessed.select('cleaned_text').rdd\n",
        "# reply_preprocessed_rdd.flatMap(lambda x: x).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovsCXQmn6_5N"
      },
      "source": [
        "# source_preprocessed = source_preprocessed.withColumn('glove_vector_source', udf(lambda : Vectors.zeros(300), VectorUDT())())\n",
        "# source_preprocessed = source_preprocessed.withColumn('bert_vector_source', udf(lambda : Vectors.zeros(768), VectorUDT())())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0egy0IJ2bEmv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAcnDpiIODUb"
      },
      "source": [
        "#eric\n",
        "#cosine similarity bert_vector and original\n",
        "# embed_bert = np.array([x for x in reply_preprocessed.select('bert_vector').collect()])\n",
        "# nsamples, nx, ny = embed_bert.shape\n",
        "# embed_bert = embed_bert.reshape((nsamples,nx*ny))\n",
        "# embed_mat2 = np.array([x for x in reply_preprocessed.select('hashtf').collect()])\n",
        "# nsamples, nx, ny = embed_mat2.shape\n",
        "# embed_mat2 = embed_mat2.reshape((nsamples,nx*ny))\n",
        "# sim_mat = cosine_similarity(embed_bert,embed_bert,embed_mat2)\n",
        "#cosine similarity glove_vector and original\n",
        "# embed_mat = np.array([x for x in reply_preprocessed.select('glove_vector').collect()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc8qL-JwLU70"
      },
      "source": [
        "# TODO sim w/ source tweet\n",
        "# TODO fix nans\n",
        "@udf('float')\n",
        "def cosine_udf(x,y):\n",
        "  return max(0,float(1 - x.dot(y)/(x.norm(2)*y.norm(2))))\n",
        "\n",
        "# reply_preprocessed = reply_preprocessed.withColumn('wrt_source_glove', cosine_udf(col('glove_vector'), col('glove_vector_source')))\n",
        "reply_preprocessed = reply_preprocessed.withColumn('wrt_source_bert', cosine_udf(col('bert_vector'), col('bert_vector_source')))\n",
        "# # reply_preprocessed = reply_preprocessed.withColumn('wrt_source_bert_large', util.pytorch_cos_sim(col('glove_vector'), col('bert_vector_other_large_source')))\n",
        "# # reply_preprocessed = reply_preprocessed.withColumn('wrt_source_bert_base', util.pytorch_cos_sim(col('bert_vector'), col('bert_vector_other_base_source')))\n",
        "# source_preprocessed = source_preprocessed.withColumn('wrt_source_glove', lit(0))\n",
        "# source_preprocessed = reply_preprocessed.withColumn('wrt_source_bert', lit(0))\n",
        "\n",
        "# TODO sim w/ thread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjcXmdMgssL"
      },
      "source": [
        "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "# # Two lists of sentences\n",
        "# sentences1 = ['The cat sits outside',\n",
        "#              'A man is playing guitar',\n",
        "#              'The new movie is awesome']\n",
        "\n",
        "# sentences2 = ['The dog plays in the garden',\n",
        "#               'A woman watches TV',\n",
        "#               'The new movie is so great']\n",
        "\n",
        "# #Compute embedding for both lists\n",
        "# embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "# embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "# #Compute cosine-similarits\n",
        "# cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "# #Output the pairs with their score\n",
        "# for i in range(len(sentences1)):\n",
        "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFq6fvgRHpO2"
      },
      "source": [
        "# reply_preprocessed.na.fill(value=0,subset=[\"wrt_source_glove\"])\n",
        "# reply_preprocessed.select('ratiocapital').take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zml62G5cHgtG"
      },
      "source": [
        "all_features = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls hasmedia\n",
        "ratiocapital charlen issource wordlen hasnegation allcapsratio hashtf\n",
        "favorite_count friends_count followers_count\"\"\".split() + list(words_dict.keys())\n",
        "# reply_features = \"glove_vector_source bert_vector_source wrt_source_glove wrt_source_bert wrt_source_bert_large wrt_source_bert_base\".split()\n",
        "# reply_features = \"glove_vector_source bert_vector_source wrt_source_glove wrt_source_bert\".split()\n",
        "reply_features = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVMw-42wbb-x",
        "outputId": "d0ba6a7b-8297-4da1-f509-999e9a7eefc9"
      },
      "source": [
        "reply_preprocessed.select(['id'] + all_features + reply_features).union(\n",
        "        source_preprocessed.select(['id'] + all_features + reply_features)).select('id').distinct().count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roxwfi6MH44q"
      },
      "source": [
        "train_tweets = reply_preprocessed.select(['id'] + all_features + reply_features).union(\n",
        "        source_preprocessed.select(['id'] + all_features + reply_features)\n",
        "    )\n",
        "train_all = train_key_taskA.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join(train_tweets, 'id'\n",
        ")\n",
        "\n",
        "dev_tweets =   reply_preprocessed.select(['id'] + all_features + reply_features).union(\n",
        "       source_preprocessed.select(['id'] + all_features + reply_features) \n",
        ")\n",
        "\n",
        "dev = dev_key_taskA.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join( dev_tweets, 'id'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfnsJ8caL8p3",
        "outputId": "b15515d6-26e6-4918-e3af-14e949efca4b"
      },
      "source": [
        "print(train_key_taskA.count())\n",
        "print(train_tweets.count())\n",
        "print(train_all.count())\n",
        "print(dev_key_taskA.count())\n",
        "print(dev_tweets.count())\n",
        "print(dev.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5217\n",
            "3629\n",
            "2899\n",
            "1485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l68eobhLwpE"
      },
      "source": [
        "train_test = train_key_taskA.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join(train_tweets, 'id', 'right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "61OKvBG6S4nu",
        "outputId": "1262f4ef-5278-46c5-d642-4a7ffefa4136"
      },
      "source": [
        "train_test.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Clo9UOLSaBme"
      },
      "source": [
        "# ML TEST\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNeX6GIcl0tK"
      },
      "source": [
        "# inputCols = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls\n",
        "# hasmedia ratiocapital charlen issource wordlen hasnegation allcapsratio wrt_source_bert\"\"\".split()\n",
        "\n",
        "inputCols = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls\n",
        "hasmedia ratiocapital charlen issource wordlen hasnegation allcapsratio\"\"\".split() + list(words_dict.keys())\n",
        "\n",
        "assembler = VectorAssembler(inputCols=inputCols,outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False)\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "pipeline = Pipeline(stages=[assembler, scaler, indexer])\n",
        "\n",
        "processor = pipeline.fit(train_all)\n",
        "\n",
        "temp = processor.transform(train_all)\n",
        "train_all_features_df = temp.select(['features', 'label_index'])\n",
        "\n",
        "temp = processor.transform(dev)\n",
        "dev_features_df = temp.select(['features', 'label_index'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytZqWtxCodVK"
      },
      "source": [
        "# layers = [\n",
        "#           train_all_features_df.schema[0].metadata[\"ml_attr\"][\"num_attrs\"],\n",
        "#           128,\n",
        "#           4\n",
        "# ]\n",
        "# trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, blockSize=128, seed=1234, labelCol='label_index')\n",
        "\n",
        "# model = trainer.fit(train_all_features_df, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm6pyFKjoevq"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "trainer = LogisticRegression(maxIter=10, regParam=0.001, labelCol='label_index')\n",
        "model = trainer.fit(train_all_features_df, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGj20JB7gbG4"
      },
      "source": [
        "# from pyspark.ml.classification import RandomForestClassifier\n",
        "# trainer = RandomForestClassifier(numTrees=10, labelCol='label_index')\n",
        "# model = trainer.fit(train_all_features_df, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlSX9B4lSlZ"
      },
      "source": [
        "\n",
        "\n",
        "# compute f1 on the dev set\n",
        "result = model.transform(dev_features_df)\n",
        "predictionAndLabels = result.select(\"prediction\", \"label_index\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='label_index')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZTNePpXiT3j",
        "outputId": "afc69c2d-d175-48c8-c1d1-8de3cfe3e316"
      },
      "source": [
        "# train_all_features_df.schema[0].metadata[\"ml_attr\"][\"num_attrs\"]\n",
        "print(\"Test set f1 = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set f1 = 0.7003150902561994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVp9ou8IlmYe",
        "outputId": "a75fd072-7e68-45c0-9845-206e42560a0a"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_true = result.select(['label_index']).collect()\n",
        "y_pred = result.select(['prediction']).collect()\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.96      0.85       500\n",
            "         1.0       0.74      0.32      0.45        87\n",
            "         2.0       0.71      0.52      0.60        81\n",
            "         3.0       0.00      0.00      0.00        62\n",
            "\n",
            "    accuracy                           0.75       730\n",
            "   macro avg       0.55      0.45      0.47       730\n",
            "weighted avg       0.69      0.75      0.70       730\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00RTiiBalazZ"
      },
      "source": [
        "model.save('subtaskA_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWcJRPNQlsc6",
        "outputId": "fbe05fed-8b99-4855-91ea-f20934f40bc6"
      },
      "source": [
        "!zip -r subtaskA_model subtaskA_model/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: subtaskA_model/ (stored 0%)\n",
            "  adding: subtaskA_model/metadata/ (stored 0%)\n",
            "  adding: subtaskA_model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: subtaskA_model/metadata/part-00000 (deflated 44%)\n",
            "  adding: subtaskA_model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: subtaskA_model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: subtaskA_model/data/ (stored 0%)\n",
            "  adding: subtaskA_model/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: subtaskA_model/data/part-00000-4f9020c3-b471-4e19-8a08-b82c8ba49617-c000.snappy.parquet (deflated 61%)\n",
            "  adding: subtaskA_model/data/.part-00000-4f9020c3-b471-4e19-8a08-b82c8ba49617-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: subtaskA_model/data/_SUCCESS (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "YvOSz8uPnxAR",
        "outputId": "849dff65-f319-4379-e52b-2a7898f47111"
      },
      "source": [
        "import pandas as pd\n",
        "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
        "    list_extract = []\n",
        "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
        "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
        "    varlist = pd.DataFrame(list_extract)\n",
        "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
        "    return(varlist.sort_values('score', ascending = False))\n",
        "ExtractFeatureImp(model.featureImportances, dev_features_df, 'features')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-e9998beaa3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvarlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvarlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatureImp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mExtractFeatureImp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatureImportances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_features_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegressionModel' object has no attribute 'featureImportances'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtOs9iYYJZ10"
      },
      "source": [
        "### ADD has_false_synonyms has_false_antonyms\n",
        "all_features = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls hasmedia\n",
        "ratiocapital charlen issource wordlen hasnegation allcapsratio hashtf\n",
        "favorite_count friends_count followers_count\"\"\".split() + list(words_dict.keys())\n",
        "# reply_features = \"glove_vector_source bert_vector_source wrt_source_glove wrt_source_bert wrt_source_bert_large wrt_source_bert_base\".split()\n",
        "# reply_features = \"glove_vector_source bert_vector_source wrt_source_glove wrt_source_bert\".split()\n",
        "reply_features = []\n",
        "\n",
        "train_tweets = reply_preprocessed.select(['id'] + all_features + reply_features).union(\n",
        "        source_preprocessed.select(['id'] + all_features + reply_features)\n",
        "    )\n",
        "train_all = train_key_taskA.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join(train_tweets, 'id'\n",
        ")\n",
        "\n",
        "dev_tweets =   reply_preprocessed.select(['id'] + all_features + reply_features).union(\n",
        "       source_preprocessed.select(['id'] + all_features + reply_features) \n",
        ")\n",
        "\n",
        "dev = dev_key_taskA.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join( dev_tweets, 'id'\n",
        ")\n",
        "\n",
        "# ML TEST\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "inputCols = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls\n",
        "hasmedia ratiocapital charlen issource wordlen hasnegation allcapsratio\"\"\".split() + list(words_dict.keys())\n",
        "\n",
        "assembler = VectorAssembler(inputCols=inputCols,outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False)\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "pipeline = Pipeline(stages=[assembler, scaler, indexer])\n",
        "\n",
        "processor = pipeline.fit(train_all)\n",
        "\n",
        "temp = processor.transform(train_all)\n",
        "train_all_features_df = temp.select(['features', 'label_index'])\n",
        "\n",
        "temp = processor.transform(dev)\n",
        "dev_features_df = temp.select(['features', 'label_index'])\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "trainer = LogisticRegression(maxIter=10, regParam=0.001, labelCol='label_index')\n",
        "model = trainer.fit(train_all_features_df, )\n",
        "\n",
        "\n",
        "# compute f1 on the dev set\n",
        "result = model.transform(dev_features_df)\n",
        "predictionAndLabels = result.select(\"prediction\", \"label_index\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='label_index')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f53San3FKdxZ"
      },
      "source": [
        "print(\"Test set f1 = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHyKZslRKmbr"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_true = result.select(['label_index']).collect()\n",
        "y_pred = result.select(['prediction']).collect()\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VarULJy6oeJj"
      },
      "source": [
        "indexer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_nF2Bkdlwks"
      },
      "source": [
        "### TESTS ##\n",
        "# df.select('hasurl').describe().show()\n",
        "# temp_df.select('hastf').take(5)\n",
        "# temp_df.select('words').take(5)\n",
        "# df.select('hasnegation').filter(df.hasnegation > 0).take(5)\n",
        "# df.select('allcapsratio').describe().show()\n",
        "# df.select('ratiocapital').describe().show()\n",
        "# df.selectExpr('CAST((in_reply_to_status_id IS NULL) AS INT)').describe().show()\n",
        "\n",
        "# df.selectExpr('size(entities[\"media\"])').describe().show()\n",
        "# df.selectExpr('size(entities[\"urls\"])').describe().show()\n",
        "# df.selectExpr('size(entities[\"hashtags\"])').describe().show()\n",
        "# df.selectExpr('size(entities[\"symbols\"])').describe().show()\n",
        "# df.selectExpr('size(entities[\"urls\"])').describe().show()\n",
        "# df.selectExpr('size(entities[\"user_mentions\"])').describe().show()\n",
        "# df.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1io2d1_gdzQ"
      },
      "source": [
        "# from sparknlp.base import *\n",
        "# from sparknlp.annotator import *\n",
        "# from pyspark.ml import Pipeline\n",
        "# documentAssembler = DocumentAssembler() \\\n",
        "#     .setInputCol(\"cleaned_text\") \\\n",
        "#     .setOutputCol(\"document\")\n",
        "\n",
        "# regexTokenizer = Tokenizer() \\\n",
        "#     .setInputCols([\"document\"]) \\\n",
        "#     .setOutputCol(\"token\")\n",
        "\n",
        "# ner_dl = WordEmbeddings.pretrained('glove_100d')\n",
        "# x_df = documentAssembler.transform(df)\n",
        "# x_df = regexTokenizer.fit(x_df).transform(x_df)\n",
        "# sentiment_detector = SentimentDetector.pretrained() \\\n",
        "#     .setInputCols([\"document\", \"token\"]) \\\n",
        "#     .setOutputCol(\"sentiment\") \\\n",
        "#     .setLazyAnnotator(False)\n",
        "# x_df = sentiment_detector.transform(x_df)\n",
        "# x_df.select('sentiment').take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_yXGeNSbO-"
      },
      "source": [
        "# reply_preprocessed.select('glove_vector').take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG9PuoFwSl2C"
      },
      "source": [
        "# reply_preprocessed.select('cleaned_text').take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEwi55bXea6J"
      },
      "source": [
        "# from pyspark.sql.functions import regexp_replace\n",
        "# df = spark.createDataFrame([('@CBCNews',)], ['str'])\n",
        "# reply_preprocessed.withColumn('cleaned_text',regexp_replace(col('cleaned_text'), r'(@([A-Za-z0-9]+))', '')).select('cleaned_text').take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP6Qd43YwTrZ"
      },
      "source": [
        "# reply_preprocessed.groupby('in_reply_to_status_id').agg(collect_list('id').alias('replies_id')).take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7duoEcqSvRn"
      },
      "source": [
        "# SentenceTransformer('average_word_embeddings_glove.6B.300d').encode('ðŸ˜±')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbhom-RxZS-Y"
      },
      "source": [
        "# reply_preprocessed.groupby('in_reply_to_status_id').agg(collect_list('wrt_source_bert').alias('avg_cos')).take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P58dXeOoz-u"
      },
      "source": [
        "all_features = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls hasmedia\n",
        "ratiocapital charlen issource wordlen hasnegation allcapsratio hashtf\n",
        "favorite_count friends_count followers_count\"\"\".split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UVDmDfsaT7C"
      },
      "source": [
        "def avg_cos(wrt_source_bert_list):\n",
        "  n = len(wrt_source_bert_list)\n",
        "  sum = 0\n",
        "  for i in wrt_source_bert_list:\n",
        "    sum += i\n",
        "  return sum/n\n",
        "\n",
        "avg_cos_udf = udf(avg_cos, FloatType())\n",
        "\n",
        "# reply_preprocessed.groupby('in_reply_to_status_id').agg(collect_list('wrt_source_bert').alias('avg_cos')).withColumn('avg_cos', avg_cos_udf('avg_cos')).take(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vapC77-3GGnF"
      },
      "source": [
        "temp = reply_preprocessed.groupBy('in_reply_to_status_id').agg({'wrt_source_bert':'avg'})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpvRiut-cMfP"
      },
      "source": [
        "def rename_cols(agg_df, ignore_first_n=1):\n",
        "    \"\"\"changes the default spark aggregate names `avg(colname)` \n",
        "    to something a bit more useful. Pass an aggregated dataframe\n",
        "    and the number of aggregation columns to ignore.\n",
        "    \"\"\"\n",
        "    delimiters = \"(\", \")\"\n",
        "    split_pattern = '|'.join(map(re.escape, delimiters))\n",
        "    splitter = partial(re.split, split_pattern)\n",
        "    split_agg = lambda x: '_'.join(splitter(x))[0:-ignore_first_n]\n",
        "    renamed = map(split_agg, agg_df.columns[ignore_first_n:])\n",
        "    renamed = zip(agg_df.columns[ignore_first_n:], renamed)\n",
        "    for old, new in renamed:\n",
        "        agg_df = agg_df.withColumnRenamed(old, new)\n",
        "    return agg_df\n",
        "temp = rename_cols(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Q18oLHbv-1"
      },
      "source": [
        "# temp = reply_preprocessed.groupby('in_reply_to_status_id').agg(collect_list('wrt_source_bert').alias('avg_cos')).withColumn('avg_cos', avg_cos_udf('avg_cos'))\n",
        "source_all_features = source_preprocessed.join(temp, source_preprocessed.id == temp.in_reply_to_status_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH472q1ZkM5c"
      },
      "source": [
        "# \tidx\tname\tscore\n",
        "# 9\t9\tissource\t0.282258\n",
        "# 4\t4\tmentions_count\t0.273590\n",
        "# 1\t1\thasqmark\t0.165910\n",
        "# 6\t6\thasmedia\t0.073971\n",
        "# 5\t5\thasurls\t0.070193\n",
        "# 8\t8\tcharlen\t0.031644\n",
        "# 10\t10\twordlen\t0.022455\n",
        "# 7\t7\tratiocapital\t0.019213\n",
        "# 2\t2\thasperiod\t0.016388\n",
        "# 3\t3\thashtags_count\t0.014601\n",
        "# 12\t12\tallcapsratio\t0.010897\n",
        "# 19\t19\thas_question_words\t0.006331\n",
        "# 11\t11\thasnegation\t0.004701\n",
        "# 16\t16\thas_knowledge\t0.003167\n",
        "# 0\t0\thasmark\t0.002603\n",
        "# 14\t14\thas_report_words\t0.001570\n",
        "# 18\t18\thas_curse_words\t0.000283\n",
        "# 13\t13\thas_belief_words\t0.000226\n",
        "# 15\t15\thas_doubt_words\t0.000000\n",
        "# 17\t17\thas_denial_words\t0.000000\n",
        "# 20\t20\thas_other_words\t0.000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_9btklLPwJ8"
      },
      "source": [
        "### TASK B \n",
        "###\n",
        "source_features = []\n",
        "\n",
        "train_all_taskb = train_key_taskB.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join(\n",
        "    source_all_features.select(['id'] + all_features + source_features), 'id'\n",
        ")\n",
        "dev_taskb = dev_key_taskB.withColumnRenamed('key', 'id').withColumnRenamed('value', 'label').join(\n",
        "    source_all_features.select(['id'] + all_features + source_features), 'id'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnBgd38T7xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12955f33-3777-4a2a-836c-5b88bf65dfe0"
      },
      "source": [
        "print(train_key_taskB.count())\n",
        "print(train_all_taskb.count())\n",
        "print(dev_key_taskB.count())\n",
        "print(dev_taskb.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "327\n",
            "296\n",
            "38\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEhzrpTKUoTl"
      },
      "source": [
        "# ML TEST\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "inputCols = \"\"\"hasmark hasqmark hasperiod hashtags_count mentions_count hasurls\n",
        "hasmedia ratiocapital charlen issource wordlen hasnegation allcapsratio hashtf\n",
        "favorite_count friends_count followers_count\"\"\".split() + source_features\n",
        "\n",
        "assembler = VectorAssembler(inputCols=inputCols,outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False)\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "pipeline = Pipeline(stages=[assembler, scaler, indexer])\n",
        "\n",
        "processor = pipeline.fit(train_all_taskb)\n",
        "\n",
        "temp = processor.transform(train_all_taskb)\n",
        "train_all_taskb_features_df = temp.select(['features', 'label_index'])\n",
        "\n",
        "temp = processor.transform(dev_taskb)\n",
        "dev_taskb_features_df = temp.select(['features', 'label_index'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXx-64cAWigA"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "trainer = LogisticRegression(maxIter=10, regParam=0.001, labelCol='label_index')\n",
        "model = trainer.fit(train_all_taskb_features_df, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnt1EOxrWm3P"
      },
      "source": [
        "result = model.transform(dev_taskb_features_df)\n",
        "predictionAndLabels = result.select(\"prediction\", \"label_index\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='label_index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItLPYyacWrCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3efe41f-a6c3-4bc3-8512-1591d3b42e14"
      },
      "source": [
        "print(\"Test set f1 = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set f1 = 0.3277504105090312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1xpxQ81jJsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df61f01-98fe-4aeb-ccb3-e7db0af01011"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_true = result.select(['label_index']).collect()\n",
        "y_pred = result.select(['prediction']).collect()\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.29      0.75      0.41         8\n",
            "         1.0       0.50      0.25      0.33         8\n",
            "         2.0       0.67      0.17      0.27        12\n",
            "\n",
            "    accuracy                           0.36        28\n",
            "   macro avg       0.48      0.39      0.34        28\n",
            "weighted avg       0.51      0.36      0.33        28\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
